# -*- coding: utf-8 -*-
"""nlp_ngram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZZ9eYGTsjSomtU8QjmLhVtPzzfltLHsD
"""

#importing libraries
import pandas as pd
import re
import numpy as np
#import nltk
#from nltk.corpus import stopwords
#nltk.download('stopwords')
from collections import Counter

#loading dataset
with open('/content/train.txt') as f:
  train = f.read()

train

#preprocessing
train = train.split('\n')
train = list(filter(lambda x: len(x)>0, train))
train = list(map(lambda x: re.sub(r'[^a-zA-Z]+', ' ', x), train))
train = list(map(lambda x: x.lower(), train))
train = list(map(lambda x: '$start ' + x + ' $end', train))
train_list = [i.split(' ') for i in train]
train_unigram = [word for list in train_list for word in list]
#train_unigram = [word for word in train_unigram if word not in stopwords.words('english')]
train_unigram = list(filter(lambda x: len(x)>0, train_unigram))

#unknown word handling train
#replace words that appear less than 6 times with $unk
count_unigrams = Counter(train_unigram)
for i in range(len(train_unigram)):
  if count_unigrams[train_unigram[i]]<=2:
    train_unigram[i] = '$unk'
train_unigram

train_bigram = [train_unigram[i] + '_' + train_unigram[i+1] for i in range(len(train_unigram)-1)]
train_bigram = list(filter(lambda x: x!='$end_$start', train_bigram))

#unigram and bigram counts from train set
count_unigrams = Counter(train_unigram)
#count_unigrams = defaultdict(lambda: 0, count_unigrams)
count_bigrams = Counter(train_bigram)
#count_bigrams = defaultdict(lambda: 0, count_bigrams)

#Vocabulary size
V = len(count_unigrams.keys())
print(V)
print(len(train_unigram))
count_unigrams['$unk']

#unsmoothed probabilities
unsmoothed_prob_unigrams = dict(map(lambda x: (x[0], x[1]/len(train_unigram)), count_unigrams.items()))
unsmoothed_prob_bigrams = dict(map(lambda x: (x[0], x[1]/count_unigrams[x[0].split("_")[0]]), count_bigrams.items()))

def smoothed_probability(word, gram=1, k=1):
  if gram ==1:
    cw = count_unigrams[word] + k
    c = len(train_unigram) + k*V
  if gram ==2:
    cw = count_bigrams[word] + k
    c = count_unigrams[word.split("_")[0]] + k*V

  p = cw/c
  return p

def perplexity(x, gram = 1, k=1):
  p=0
  n = len(x)
  for i in x:
    p += np.log(smoothed_probability(i, gram, k))
  p = p/n
  pp = np.exp(-p)
  return pp

with open('/content/val.txt') as f:
  val = f.read()

val

val = val.split('\n')
val = list(filter(lambda x: len(x)>0, val))
val = list(map(lambda x: re.sub(r'[^a-zA-Z]+', ' ', x), val))
val = list(map(lambda x: x.lower(), val))
val = list(map(lambda x: '$start ' + x + ' $end', val))
val_list = [i.split(' ') for i in val]
val_unigram = [word for list in val_list for word in list]
#val_unigram = [word for word in val_unigram if word not in stopwords.words('english')]
val_unigram = list(filter(lambda x: len(x)>0, val_unigram))

#unknown word handling in val
for i in range(len(val_unigram)):
  if val_unigram[i] not in train_unigram:
    val_unigram[i] = '$unk'

val_bigram = [val_unigram[i] + '_' + val_unigram[i+1] for i in range(len(val_unigram)-1)]
val_bigram = list(filter(lambda x: x!='$end_$start', val_bigram))

#perplexity for different k-smoothing
#k=1 -> laplace
pp_unigram_val = []
pp_bigram_val = []

#val set perplexities
for k in [0, 1, 0.5, 0.1, 0.05, 0.01, 0.005]:
  pp_unigram_val.append(perplexity(val_unigram, gram = 1, k = k))
  pp_bigram_val.append(perplexity(val_bigram, gram = 2, k = k))

pp_unigram_train = []
pp_bigram_train = []

#train set perplexities
for k in [0, 1, 0.5, 0.1, 0.05, 0.01, 0.005]:
  pp_unigram_train.append(perplexity(train_unigram, gram = 1, k = k))
  pp_bigram_train.append(perplexity(train_bigram, gram = 2, k = k))

pp_unigram_train

pp_bigram_train

pp_unigram_val

pp_bigram_val

#best k is 0.05, 0.01
