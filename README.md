# N-Gram Language Model

This project implements a unigram and bigram language model in Python. The model is capable of calculating the probability of sequences and evaluating their perplexity.

## Features

- Preprocesses text data to clean and tokenize sentences.
- Constructs unigram and bigram models with optional smoothing.
- Calculates unsmoothed and smoothed probabilities for n-grams.
- Evaluates model performance using perplexity.
